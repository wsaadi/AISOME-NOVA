# üîß Core Connectors - Documentation

## Vue d'ensemble

Les **Core Connectors** sont les services centraux de la plateforme qui fournissent l'acc√®s aux diff√©rents fournisseurs d'IA g√©n√©rative. Ils exposent des API REST standardis√©es permettant aux agents de communiquer avec Mistral AI, OpenAI et d'autres providers.

## Connecteurs disponibles

### 1. [Mistral AI Connector](./MISTRAL_CONNECTOR.md)
**Port:** 8005
**Statut:** Principal et recommand√©

Connecteur central pour Mistral AI offrant :
- Chat Completion (mistral-small, medium, large)
- Embeddings (mistral-embed)
- Support vision multimodale (pixtral)
- Gestion des mod√®les
- Authentification globale ou par requ√™te

**Utilisation:**
```bash
curl -X POST "http://localhost:8005/api/v1/mistral/chat" \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "Hello!"}]}'
```

### 2. [OpenAI Connector](./OPENAI_CONNECTOR.md)
**Port:** 8006
**Statut:** Optionnel

Connecteur alternatif pour OpenAI offrant :
- Chat Completion (GPT-3.5, GPT-4, GPT-4-turbo)
- Embeddings (text-embedding-ada-002)
- Support multi-mod√®les
- Compatible avec l'√©cosyst√®me OpenAI

**Utilisation:**
```bash
curl -X POST "http://localhost:8006/api/v1/openai/chat" \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "Hello!"}], "model": "gpt-4"}'
```

## Architecture des connectors

### Conception commune

Tous les connecteurs suivent la m√™me architecture :

```
core/{connector-name}/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Application FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Sch√©mas Pydantic
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Logique m√©tier
‚îÇ   ‚îî‚îÄ‚îÄ routers/             # Endpoints API
‚îú‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ requirements.txt
```

### Principes de conception

1. **Interface unifi√©e** : API REST standardis√©e pour tous les providers
2. **Authentification flexible** : Cl√© globale ou par requ√™te via header `X-API-Key`
3. **Validation stricte** : Pydantic pour toutes les entr√©es/sorties
4. **Gestion d'erreurs** : R√©ponses standardis√©es en cas d'√©chec
5. **Logs d√©taill√©s** : Tra√ßabilit√© compl√®te des requ√™tes

## Configuration

### Variables d'environnement

Chaque connector n√©cessite sa propre configuration :

#### Mistral Connector
```bash
MISTRAL_API_KEY=your_mistral_key        # Obligatoire
MISTRAL_DEFAULT_MODEL=mistral-small-latest
MISTRAL_DEFAULT_TEMPERATURE=0.7
ENVIRONMENT=production
CORS_ORIGINS=*
```

#### OpenAI Connector
```bash
OPENAI_API_KEY=your_openai_key          # Obligatoire
OPENAI_DEFAULT_MODEL=gpt-3.5-turbo
OPENAI_DEFAULT_TEMPERATURE=0.7
ENVIRONMENT=production
CORS_ORIGINS=*
```

### D√©marrage

```bash
# D√©marrer tous les connectors
docker-compose up -d mistral-connector openai-connector

# Ou individuellement
docker-compose up -d mistral-connector

# V√©rifier l'√©tat
docker-compose ps | grep connector

# Tester les services
curl http://localhost:8005/health
curl http://localhost:8006/health
```

## Utilisation

### Depuis Python

```python
import httpx

async def use_mistral():
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8005/api/v1/mistral/chat",
            json={
                "messages": [
                    {"role": "user", "content": "Bonjour!"}
                ]
            }
        )
        return response.json()

async def use_openai():
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8006/api/v1/openai/chat",
            json={
                "messages": [
                    {"role": "user", "content": "Hello!"}
                ],
                "model": "gpt-4"
            }
        )
        return response.json()
```

### Depuis un agent

Les agents utilisent les connectors pour leurs op√©rations IA :

```python
# Agent utilisant Mistral
mistral_response = await httpx.post(
    "http://mistral-connector:8000/api/v1/mistral/chat",
    json={"messages": conversation_history}
)

# Fallback vers OpenAI si besoin
if not mistral_response.json()["success"]:
    openai_response = await httpx.post(
        "http://openai-connector:8000/api/v1/openai/chat",
        json={"messages": conversation_history}
    )
```

## Comparaison des providers

| Crit√®re | Mistral AI | OpenAI |
|---------|-----------|--------|
| **Co√ªt** | G√©n√©ralement moins cher | Plus cher (GPT-4) |
| **Vitesse** | Tr√®s rapide | Variable |
| **Fran√ßais** | Excellent | Bon |
| **Contexte** | 32K tokens | Jusqu'√† 128K |
| **Open Source** | Mod√®les ouverts disponibles | Propri√©taire |
| **Recommand√© pour** | Usage g√©n√©ral, fran√ßais | Cas sp√©cifiques, anglais |

## Monitoring

### Health checks

```bash
# Mistral Connector
curl http://localhost:8005/health
# Retourne: {"status": "healthy", "mistral_configured": true}

# OpenAI Connector
curl http://localhost:8006/health
# Retourne: {"status": "healthy", "openai_configured": true}
```

### Logs

```bash
# Logs en temps r√©el
docker-compose logs -f mistral-connector
docker-compose logs -f openai-connector

# Logs des derni√®res erreurs
docker-compose logs --tail=100 mistral-connector | grep ERROR
```

## Troubleshooting

### Probl√®mes courants

#### Service ne d√©marre pas
```bash
# V√©rifier les logs
docker-compose logs mistral-connector

# V√©rifier la cl√© API
cat .env | grep API_KEY

# Rebuild
docker-compose up -d --build mistral-connector
```

#### Erreur 401 (Unauthorized)
```bash
# La cl√© API est invalide ou manquante
# V√©rifier et mettre √† jour dans .env
echo "MISTRAL_API_KEY=nouvelle_cle" >> .env
docker-compose restart mistral-connector
```

#### Timeout
```python
# Augmenter le timeout client
async with httpx.AsyncClient(timeout=60.0) as client:
    response = await client.post(...)
```

## S√©curit√©

### Bonnes pratiques

1. ‚úÖ **Cl√©s API** : Toujours dans .env, jamais dans le code
2. ‚úÖ **CORS** : Restreindre en production (`CORS_ORIGINS=https://votredomaine.com`)
3. ‚úÖ **HTTPS** : Utiliser un reverse proxy en production
4. ‚úÖ **Rate limiting** : Impl√©menter des limites par utilisateur
5. ‚úÖ **Monitoring** : Surveiller l'usage et les co√ªts

## Documentation d√©taill√©e

- [Mistral AI Connector - Documentation compl√®te](./MISTRAL_CONNECTOR.md)
- [OpenAI Connector - Documentation compl√®te](./OPENAI_CONNECTOR.md)
- [Documentation plateforme](../platform/PLATFORM.md)

## Liens externes

- [Mistral AI Documentation](https://docs.mistral.ai/)
- [OpenAI API Documentation](https://platform.openai.com/docs/)

---

**Derni√®re mise √† jour** : Janvier 2026
