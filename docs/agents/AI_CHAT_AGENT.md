# ğŸ¤– AI Chat Agent

## ğŸ“‹ Vue d'ensemble

L'**AI Chat Agent** est un agent orchestrateur qui fournit une interface de chat IA gouvernÃ©e et sÃ©curisÃ©e. Il combine modÃ©ration de contenu, classification professionnelle et gÃ©nÃ©ration de rÃ©ponses IA pour offrir une expÃ©rience similaire Ã  ChatGPT mais contrÃ´lÃ©e pour un usage professionnel.

### Objectif

Fournir un chat IA professionnel avec :
- **Gouvernance stricte** : ModÃ©ration et classification systÃ©matiques
- **Multimodal** : Support texte, images (JPG, PNG, GIF) et documents
- **Multi-providers** : Compatible Mistral AI et OpenAI
- **Contexte maintenu** : Historique de conversation avec fichiers

### CapacitÃ©s

- ğŸ’¬ **Chat multimodal** : Texte, images, documents (TXT, MD, JSON, CSV)
- ğŸ›¡ï¸ **ModÃ©ration** : DÃ©tection de contenu inappropriÃ©
- ğŸ¯ **Classification** : Validation du caractÃ¨re professionnel
- ğŸ‘ï¸ **Vision** : Analyse d'images avec modÃ¨les vision (Pixtral)
- ğŸ“š **Historique** : Maintien du contexte conversationnel

## ğŸ—ï¸ Architecture

### Workflow de traitement

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. GATEKEEPER : Classification rapide              â”‚
â”‚    [Content Classification Tool]                    â”‚
â”‚    â†’ Petit modÃ¨le rapide                           â”‚
â”‚    â†’ Bloque si non professionnel                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ Si professionnel
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. MODÃ‰RATION : VÃ©rification approfondie           â”‚
â”‚    [Prompt Moderation Tool]                         â”‚
â”‚    â†’ DÃ©tecte insultes, contenu sensible            â”‚
â”‚    â†’ Analyse en profondeur                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ Si approuvÃ©
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. GÃ‰NÃ‰RATION : RÃ©ponse IA                         â”‚
â”‚    [Mistral/OpenAI Connector]                       â”‚
â”‚    â†’ GÃ©nÃ¨re rÃ©ponse contextuelle                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DÃ©pendances

- **Content Classification Tool** (port 8014) - Classification professionnelle
- **Prompt Moderation Tool** (port 8013) - ModÃ©ration de contenu
- **Mistral Connector** (port 8005) - Provider IA principal
- **OpenAI Connector** (port 8006) - Provider IA alternatif

### Structure du service

```
agents/ai-chat-agent/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # Application FastAPI
â”‚   â”œâ”€â”€ config.py            # Configuration
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ chat_models.py   # SchÃ©mas requÃªtes/rÃ©ponses
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ orchestrator.py  # Logique d'orchestration
â”‚   â”‚   â”œâ”€â”€ moderation.py    # Client modÃ©ration
â”‚   â”‚   â””â”€â”€ classification.py # Client classification
â”‚   â””â”€â”€ routers/
â”‚       â””â”€â”€ chat.py          # Endpoints API
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”Œ API REST

### Endpoint principal

#### **POST /api/v1/chat/completions**

Chat avec modÃ©ration et classification.

**RequÃªte simple (texte):**
```json
{
  "messages": [
    {
      "role": "user",
      "content": "Analyse notre performance Q4"
    }
  ],
  "provider": "mistral",
  "model": "mistral-small-latest",
  "temperature": 0.7,
  "max_tokens": 4096,
  "strict_moderation": true,
  "minimum_professional_score": 60.0
}
```

**RequÃªte multimodale (avec images):**
```json
{
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "Analyse ce graphique de ventes"
        },
        {
          "type": "image_url",
          "image_url": {
            "url": "data:image/jpeg;base64,/9j/4AAQSkZJRg..."
          }
        }
      ]
    }
  ],
  "provider": "mistral",
  "model": "pixtral-12b-2409",
  "temperature": 0.7
}
```

**RÃ©ponse:**
```json
{
  "success": true,
  "message": {
    "role": "assistant",
    "content": "Voici l'analyse de vos ventes Q4...",
    "model": "mistral-small-latest"
  },
  "moderation": {
    "passed": true,
    "professional": true,
    "professional_score": 95.0,
    "warnings": []
  },
  "usage": {
    "prompt_tokens": 150,
    "completion_tokens": 200,
    "total_tokens": 350
  }
}
```

**RÃ©ponse en cas de blocage:**
```json
{
  "success": false,
  "error": "Content blocked",
  "reason": "Non-professional content detected",
  "moderation": {
    "passed": false,
    "professional": false,
    "professional_score": 25.0,
    "warnings": ["Personal content detected"]
  }
}
```

### ParamÃ¨tres de configuration

| ParamÃ¨tre | Type | Description | DÃ©faut |
|-----------|------|-------------|--------|
| `messages` | Array | **Obligatoire**. Messages de conversation | - |
| `provider` | String | Provider IA (`mistral` ou `openai`) | mistral |
| `model` | String | ModÃ¨le Ã  utiliser | mistral-small-latest |
| `temperature` | Float (0-2) | ContrÃ´le de crÃ©ativitÃ© | 0.7 |
| `max_tokens` | Integer | Limite tokens rÃ©ponse | 4096 |
| `strict_moderation` | Boolean | Mode modÃ©ration stricte | true |
| `minimum_professional_score` | Float | Score minimum (0-100) | 60.0 |
| `skip_classification` | Boolean | Passer la classification | false |
| `skip_moderation` | Boolean | Passer la modÃ©ration | false |

## ğŸš€ Utilisation

### Configuration

```bash
# Variables d'environnement (.env)
AI_CHAT_ENVIRONMENT=production
AI_CHAT_CORS_ORIGINS=*

# URLs des services
MISTRAL_CONNECTOR_URL=http://mistral-connector:8000
OPENAI_CONNECTOR_URL=http://openai-connector:8000
PROMPT_MODERATION_URL=http://prompt-moderation-tool:8000
CONTENT_CLASSIFICATION_URL=http://content-classification-tool:8000

# Configuration de modÃ©ration
DEFAULT_MINIMUM_PROFESSIONAL_SCORE=60.0
DEFAULT_STRICT_MODERATION=true
```

### DÃ©marrage

```bash
# Via Docker Compose
docker-compose up -d ai-chat-agent

# Logs
docker-compose logs -f ai-chat-agent

# Test
curl http://localhost:8012/health
```

### Exemples d'utilisation

#### Chat simple

```python
import httpx

async def chat_simple():
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8012/api/v1/chat/completions",
            json={
                "messages": [
                    {
                        "role": "user",
                        "content": "RÃ©dige un email professionnel"
                    }
                ],
                "provider": "mistral",
                "temperature": 0.7
            }
        )
        result = response.json()
        if result["success"]:
            print(result["message"]["content"])
        else:
            print(f"BloquÃ©: {result['reason']}")
```

#### Conversation multi-tours

```python
async def conversation():
    conversation_history = []

    # Premier message
    conversation_history.append({
        "role": "user",
        "content": "Explique le concept de microservices"
    })

    response1 = await client.post(
        "http://localhost:8012/api/v1/chat/completions",
        json={"messages": conversation_history}
    )

    # Ajouter la rÃ©ponse
    if response1.json()["success"]:
        conversation_history.append(response1.json()["message"])

        # Question de suivi
        conversation_history.append({
            "role": "user",
            "content": "Quels sont les avantages ?"
        })

        response2 = await client.post(
            "http://localhost:8012/api/v1/chat/completions",
            json={"messages": conversation_history}
        )
```

#### Chat avec image

```python
import base64

async def chat_with_image(image_path: str):
    # Encoder l'image en base64
    with open(image_path, "rb") as image_file:
        encoded = base64.b64encode(image_file.read()).decode()

    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8012/api/v1/chat/completions",
            json={
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "Analyse cette image"
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{encoded}"
                                }
                            }
                        ]
                    }
                ],
                "provider": "mistral",
                "model": "pixtral-12b-2409"
            }
        )
```

#### Comparaison Mistral vs OpenAI

```python
async def compare_providers(prompt: str):
    # Mistral
    mistral_response = await client.post(
        "http://localhost:8012/api/v1/chat/completions",
        json={
            "messages": [{"role": "user", "content": prompt}],
            "provider": "mistral",
            "model": "mistral-small-latest"
        }
    )

    # OpenAI
    openai_response = await client.post(
        "http://localhost:8012/api/v1/chat/completions",
        json={
            "messages": [{"role": "user", "content": prompt}],
            "provider": "openai",
            "model": "gpt-4"
        }
    )

    print("Mistral:", mistral_response.json()["message"]["content"])
    print("OpenAI:", openai_response.json()["message"]["content"])
```

## âš™ï¸ Configuration avancÃ©e

### Niveaux de modÃ©ration

#### ModÃ©ration stricte (par dÃ©faut)
```json
{
  "strict_moderation": true,
  "minimum_professional_score": 60.0
}
```
- Bloque contenu personnel
- Exige score professionnel â‰¥ 60
- DÃ©tecte donnÃ©es sensibles

#### ModÃ©ration souple
```json
{
  "strict_moderation": false,
  "minimum_professional_score": 40.0
}
```
- Avertissements seulement
- Seuil professionnel plus bas
- Plus permissif

#### Sans modÃ©ration (dÃ©conseillÃ©)
```json
{
  "skip_moderation": true,
  "skip_classification": true
}
```
- Aucune vÃ©rification
- RÃ©servÃ© au dÃ©veloppement

### ModÃ¨les supportÃ©s

**Mistral AI:**
- `mistral-small-latest` - Texte, rapide
- `mistral-medium-latest` - Texte, Ã©quilibrÃ©
- `mistral-large-latest` - Texte, prÃ©cis
- `pixtral-12b-2409` - Vision multimodale

**OpenAI:**
- `gpt-3.5-turbo` - Texte, rapide
- `gpt-4` - Texte, prÃ©cis
- `gpt-4-turbo` - Texte, contexte Ã©tendu
- `gpt-4o` - Multimodal

## ğŸ› Troubleshooting

### Erreurs courantes

#### Content blocked

**Message:** "Non-professional content detected"

**Solutions:**
- Reformuler le prompt en termes professionnels
- RÃ©duire `minimum_professional_score`
- Utiliser `skip_classification=true` (dev seulement)

#### Service unavailable

**Cause:** Tool de modÃ©ration/classification indisponible

**Solutions:**
```bash
# VÃ©rifier les services dÃ©pendants
docker-compose ps | grep moderation
docker-compose ps | grep classification

# RedÃ©marrer si nÃ©cessaire
docker-compose restart prompt-moderation-tool
docker-compose restart content-classification-tool
```

#### Timeout

**Solutions:**
```python
# Augmenter timeout client
async with httpx.AsyncClient(timeout=120.0) as client:
    response = await client.post(...)

# RÃ©duire max_tokens
{"max_tokens": 1000}  # Au lieu de 4096
```

## ğŸ”’ SÃ©curitÃ©

### FonctionnalitÃ©s de sÃ©curitÃ©

1. âœ… **Double filtrage** : Classification + ModÃ©ration
2. âœ… **DÃ©tection donnÃ©es sensibles** : Mots de passe, cartes, etc.
3. âœ… **Logs d'audit** : TraÃ§abilitÃ© des requÃªtes
4. âœ… **Validation stricte** : Pydantic pour toutes les entrÃ©es
5. âœ… **Isolation** : Chaque tool dans son container

### Recommandations

- [ ] Activer `strict_moderation` en production
- [ ] Configurer `minimum_professional_score â‰¥ 60`
- [ ] ImplÃ©menter rate limiting par utilisateur
- [ ] Logger toutes les requÃªtes bloquÃ©es
- [ ] Monitorer les taux de blocage

## ğŸ“Š Cas d'usage

### âœ… Cas d'usage professionnels acceptÃ©s

- RÃ©daction de documents professionnels
- Analyse de donnÃ©es mÃ©tier
- Support technique
- GÃ©nÃ©ration de code
- Traduction de documents
- RÃ©sumÃ© de rÃ©unions

### âŒ Cas d'usage bloquÃ©s

- Conversations personnelles
- Contenu inappropriÃ©
- DonnÃ©es sensibles (mots de passe, CB)
- Contenu non professionnel
- Spam ou abus

## ğŸ“š Ressources

### Liens internes

- [Mistral Connector](../core/MISTRAL_CONNECTOR.md)
- [OpenAI Connector](../core/OPENAI_CONNECTOR.md)
- [Prompt Moderation Tool](../tools/PROMPT_MODERATION_TOOL.md)
- [Content Classification Tool](../tools/CONTENT_CLASSIFICATION_TOOL.md)

### Documentation externe

- [Mistral AI Docs](https://docs.mistral.ai/)
- [OpenAI Docs](https://platform.openai.com/docs/)

---

**Service** : ai-chat-agent
**Port** : 8012
**Version** : 1.0.0
**DerniÃ¨re mise Ã  jour** : Janvier 2026
