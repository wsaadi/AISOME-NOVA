# üß† OpenAI Connector

## üìã Vue d'ensemble

Le **OpenAI Connector** est un service central optionnel de la plateforme pour l'interaction avec OpenAI (GPT-3.5, GPT-4, etc.). Il fournit une interface REST unifi√©e alternative au Mistral Connector, permettant aux agents de choisir entre diff√©rents fournisseurs d'IA.

### Objectif

Offrir une alternative ou un compl√©ment √† Mistral AI pour :
- Exploiter les mod√®les GPT d'OpenAI
- Comparer les performances entre fournisseurs
- Permettre le fallback entre plusieurs IA
- Supporter les cas d'usage sp√©cifiques √† OpenAI

### Capacit√©s

- üí¨ **Chat Completion** : Conversations avec GPT-3.5, GPT-4, etc.
- üî¢ **Embeddings** : Vectorisation avec text-embedding-ada-002
- üìä **Gestion des mod√®les** : Liste des mod√®les OpenAI disponibles
- üîê **Authentification flexible** : Cl√© globale ou par requ√™te

## üèóÔ∏è Architecture

### Structure du service

```
core/openai-connector/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Application FastAPI principale
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configuration et variables d'environnement
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ openai_models.py    # Sch√©mas Pydantic
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ openai_service.py   # Logique m√©tier et client OpenAI
‚îÇ   ‚îî‚îÄ‚îÄ routers/
‚îÇ       ‚îî‚îÄ‚îÄ openai.py        # Endpoints API REST
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_openai.py
‚îú‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ requirements.txt
```

### D√©pendances

```python
# requirements.txt
fastapi==0.100+           # Framework web
uvicorn==0.23+            # Serveur ASGI
openai==1.0+              # SDK officiel OpenAI
pydantic==2.0+            # Validation de donn√©es
python-dotenv==1.0+       # Variables d'environnement
```

### Flux de communication

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     HTTP/REST      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Agent   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ   OpenAI        ‚îÇ
‚îÇ  ou Tool ‚îÇ                     ‚îÇ  Connector      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                          ‚îÇ
                                          ‚îÇ SDK OpenAI
                                          ‚îÇ
                                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                   ‚îÇ  OpenAI API ‚îÇ
                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üîå API REST

### Endpoints disponibles

#### **1. Chat Completion**

```http
POST /api/v1/openai/chat
Content-Type: application/json
X-API-Key: optional_custom_key

{
  "messages": [
    {"role": "system", "content": "Tu es un assistant utile"},
    {"role": "user", "content": "Explique la photosynth√®se"}
  ],
  "model": "gpt-3.5-turbo",
  "temperature": 0.7,
  "max_tokens": 1024,
  "top_p": 1.0,
  "frequency_penalty": 0.0,
  "presence_penalty": 0.0
}
```

**R√©ponse:**
```json
{
  "success": true,
  "message": {
    "role": "assistant",
    "content": "La photosynth√®se est le processus...",
    "model": "gpt-3.5-turbo"
  },
  "usage": {
    "prompt_tokens": 25,
    "completion_tokens": 150,
    "total_tokens": 175
  }
}
```

#### **2. Embeddings**

```http
POST /api/v1/openai/embeddings
Content-Type: application/json

{
  "input": [
    "Premier texte √† vectoriser",
    "Deuxi√®me texte"
  ],
  "model": "text-embedding-ada-002"
}
```

**R√©ponse:**
```json
{
  "success": true,
  "embeddings": [
    [0.123, -0.456, 0.789, ...],
    [-0.321, 0.654, -0.987, ...]
  ],
  "model": "text-embedding-ada-002",
  "usage": {
    "total_tokens": 42
  }
}
```

#### **3. Liste des mod√®les**

```http
GET /api/v1/openai/models
```

**R√©ponse:**
```json
{
  "success": true,
  "models": [
    {
      "id": "gpt-3.5-turbo",
      "description": "Rapide et √©conomique"
    },
    {
      "id": "gpt-4",
      "description": "Plus puissant et pr√©cis"
    }
  ]
}
```

#### **4. Health check**

```http
GET /health
```

**R√©ponse:**
```json
{
  "status": "healthy",
  "service": "openai-connector",
  "version": "1.0.0",
  "openai_configured": true
}
```

### Mod√®les disponibles

| Mod√®le | Tokens max | Cas d'usage | Co√ªt relatif |
|--------|-----------|-------------|--------------|
| `gpt-3.5-turbo` | 16K | Usage g√©n√©ral, rapide | ‚≠ê |
| `gpt-3.5-turbo-16k` | 16K | Contexte √©tendu | ‚≠ê‚≠ê |
| `gpt-4` | 8K | Raisonnement avanc√©, pr√©cision | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `gpt-4-turbo` | 128K | Contexte tr√®s √©tendu | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| `gpt-4o` | 128K | Optimis√© pour vitesse et co√ªt | ‚≠ê‚≠ê‚≠ê |
| `text-embedding-ada-002` | 8K | Vectorisation, recherche | ‚≠ê |

## üöÄ Utilisation

### Configuration

1. **Cr√©er le fichier `.env`** (racine du projet)

```bash
# Ajouter dans le .env principal
OPENAI_API_KEY=sk-your_openai_api_key_here
OPENAI_ENVIRONMENT=production
OPENAI_CORS_ORIGINS=*
OPENAI_DEFAULT_MODEL=gpt-3.5-turbo
OPENAI_DEFAULT_MAX_TOKENS=1024
OPENAI_DEFAULT_TEMPERATURE=0.7
```

2. **Obtenir une cl√© API OpenAI**

Visitez https://platform.openai.com/api-keys et cr√©ez une cl√© API.

### D√©marrage

#### Via Docker Compose (recommand√©)

```bash
# Depuis la racine du projet
docker-compose up -d openai-connector

# V√©rifier les logs
docker-compose logs -f openai-connector

# Tester le service
curl http://localhost:8006/health
```

#### En d√©veloppement local

```bash
cd core/openai-connector

# Installer les d√©pendances
pip install -r requirements.txt

# Lancer le serveur
uvicorn app.main:app --reload --port 8000

# Le service sera disponible sur http://localhost:8000
```

### Exemples d'int√©gration

#### Depuis Python

```python
import httpx

OPENAI_CONNECTOR_URL = "http://localhost:8006"

# Chat avec GPT-4
async def chat_gpt4():
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{OPENAI_CONNECTOR_URL}/api/v1/openai/chat",
            json={
                "messages": [
                    {"role": "user", "content": "Explique la relativit√©"}
                ],
                "model": "gpt-4",
                "temperature": 0.7
            }
        )
        result = response.json()
        if result["success"]:
            print(result["message"]["content"])

# G√©n√©ration d'embeddings
async def generate_embeddings():
    documents = [
        "Machine learning is a subset of AI",
        "Deep learning uses neural networks",
        "Natural language processing handles text"
    ]

    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{OPENAI_CONNECTOR_URL}/api/v1/openai/embeddings",
            json={
                "input": documents,
                "model": "text-embedding-ada-002"
            }
        )
        result = response.json()
        if result["success"]:
            embeddings = result["embeddings"]
            print(f"Dimension: {len(embeddings[0])}")

# Comparaison avec Mistral
async def compare_providers():
    prompt = "Qu'est-ce que l'intelligence artificielle ?"

    # OpenAI
    openai_response = await client.post(
        "http://localhost:8006/api/v1/openai/chat",
        json={"messages": [{"role": "user", "content": prompt}]}
    )

    # Mistral
    mistral_response = await client.post(
        "http://localhost:8005/api/v1/mistral/chat",
        json={"messages": [{"role": "user", "content": prompt}]}
    )

    print("OpenAI:", openai_response.json()["message"]["content"])
    print("Mistral:", mistral_response.json()["message"]["content"])
```

#### Depuis JavaScript/TypeScript

```typescript
import { HttpClient } from '@angular/common/http';
import { Injectable } from '@angular/core';

@Injectable({ providedIn: 'root' })
export class OpenAIService {
  private baseUrl = 'http://localhost:8006/api/v1/openai';

  constructor(private http: HttpClient) {}

  chatGPT4(messages: any[]) {
    return this.http.post(`${this.baseUrl}/chat`, {
      messages,
      model: 'gpt-4',
      temperature: 0.7
    });
  }

  generateEmbeddings(texts: string[]) {
    return this.http.post(`${this.baseUrl}/embeddings`, {
      input: texts,
      model: 'text-embedding-ada-002'
    });
  }
}
```

## ‚öôÔ∏è Configuration avanc√©e

### Variables d'environnement

| Variable | Description | D√©faut | Obligatoire |
|----------|-------------|--------|-------------|
| `OPENAI_API_KEY` | Cl√© API OpenAI | - | ‚úÖ |
| `ENVIRONMENT` | Environnement | production | ‚ùå |
| `CORS_ORIGINS` | Origines CORS | * | ‚ùå |
| `DEFAULT_MODEL` | Mod√®le par d√©faut | gpt-3.5-turbo | ‚ùå |
| `DEFAULT_MAX_TOKENS` | Limite de tokens | 1024 | ‚ùå |
| `DEFAULT_TEMPERATURE` | Temp√©rature | 0.7 | ‚ùå |

### Param√®tres de requ√™te

#### Chat Completion

| Param√®tre | Type | Description | D√©faut |
|-----------|------|-------------|--------|
| `messages` | Array | **Obligatoire**. Messages | - |
| `model` | String | Mod√®le OpenAI | gpt-3.5-turbo |
| `temperature` | Float (0-2) | Cr√©ativit√© | 0.7 |
| `max_tokens` | Integer | Limite de tokens | 1024 |
| `top_p` | Float (0-1) | Nucleus sampling | 1.0 |
| `frequency_penalty` | Float (-2 √† 2) | P√©nalit√© fr√©quence | 0.0 |
| `presence_penalty` | Float (-2 √† 2) | P√©nalit√© pr√©sence | 0.0 |

## üêõ Troubleshooting

### Probl√®mes courants

#### 1. Erreur "Invalid API Key"

**Solutions:**
```bash
# V√©rifier la cl√© dans .env
cat .env | grep OPENAI_API_KEY

# Tester la cl√© directement
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"

# G√©n√©rer une nouvelle cl√© sur platform.openai.com
```

#### 2. Erreur 429 (Rate Limit)

**Cause:** Quota d√©pass√© ou trop de requ√™tes

**Solutions:**
- V√©rifier votre usage sur platform.openai.com/usage
- Passer √† un tier sup√©rieur
- Impl√©menter un syst√®me de retry avec backoff
- Utiliser un cache pour les requ√™tes r√©p√©titives

#### 3. Timeout

**Solutions:**
```python
# Augmenter le timeout
async with httpx.AsyncClient(timeout=120.0) as client:
    response = await client.post(...)

# R√©duire max_tokens
{"messages": [...], "max_tokens": 500}
```

## üîí S√©curit√©

### Bonnes pratiques

1. ‚úÖ Cl√© API en variable d'environnement
2. ‚úÖ Validation Pydantic
3. ‚úÖ CORS configur√©
4. ‚úÖ Gestion d'erreurs compl√®te
5. ‚úÖ Logs structur√©s

### Recommandations production

- [ ] HTTPS avec certificats SSL/TLS
- [ ] Restreindre CORS_ORIGINS
- [ ] Rate limiting
- [ ] Secrets Docker pour la cl√© API
- [ ] Monitoring (co√ªt, usage, performance)

## üìä Comparaison Mistral vs OpenAI

| Crit√®re | Mistral | OpenAI |
|---------|---------|--------|
| **Co√ªt** | G√©n√©ralement moins cher | Plus cher (surtout GPT-4) |
| **Vitesse** | Tr√®s rapide | Variable selon mod√®le |
| **Multilingue** | Excellent en fran√ßais | Bon mais moins performant |
| **Contexte** | 32K tokens | Jusqu'√† 128K (GPT-4-turbo) |
| **Open source** | Mod√®les ouverts disponibles | Propri√©taire |
| **Disponibilit√©** | R√©cent, en croissance | Mature, tr√®s stable |

## üìö Ressources

### Documentation officielle

- [OpenAI API Documentation](https://platform.openai.com/docs/)
- [OpenAI Models](https://platform.openai.com/docs/models)
- [OpenAI Pricing](https://openai.com/pricing)

### Liens internes

- [Documentation plateforme](../platform/PLATFORM.md)
- [Mistral Connector](./MISTRAL_CONNECTOR.md)
- [Documentation agents](../agents/)

---

**Service** : openai-connector
**Port** : 8006
**Version** : 1.0.0
**Derni√®re mise √† jour** : Janvier 2026
