# ü§ñ Mistral AI Connector

## üìã Vue d'ensemble

Le **Mistral AI Connector** est le service central de la plateforme pour l'interaction avec Mistral AI. Il fournit une interface REST unifi√©e permettant √† tous les agents et outils de la plateforme d'exploiter les capacit√©s de l'IA g√©n√©rative Mistral.

### Objectif

Centraliser et standardiser l'acc√®s √† Mistral AI pour :
- √âviter la duplication de code d'int√©gration
- G√©rer de mani√®re centralis√©e l'authentification API
- Offrir une abstraction coh√©rente pour tous les agents
- Faciliter la maintenance et les mises √† jour

### Capacit√©s

- üí¨ **Chat Completion** : Conversations contextuelles multi-tours
- üî¢ **Embeddings** : Vectorisation de texte pour recherche s√©mantique
- üìä **Gestion des mod√®les** : Liste et informations des mod√®les disponibles
- üîê **Authentification flexible** : Cl√© globale ou par requ√™te

## üèóÔ∏è Architecture

### Structure du service

```
core/mistral-connector/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Application FastAPI principale
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configuration et variables d'environnement
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mistral_models.py   # Sch√©mas Pydantic (requ√™tes/r√©ponses)
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mistral_service.py  # Logique m√©tier et client Mistral
‚îÇ   ‚îî‚îÄ‚îÄ routers/
‚îÇ       ‚îî‚îÄ‚îÄ mistral.py       # Endpoints API REST
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_mistral.py
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ API.md
‚îú‚îÄ‚îÄ Dockerfile               # Image Docker
‚îú‚îÄ‚îÄ requirements.txt         # D√©pendances Python
‚îú‚îÄ‚îÄ .env.example            # Template de configuration
‚îî‚îÄ‚îÄ README.md
```

### D√©pendances

```python
# requirements.txt
fastapi==0.100+           # Framework web
uvicorn==0.23+            # Serveur ASGI
mistralai==0.1+           # SDK officiel Mistral AI
pydantic==2.0+            # Validation de donn√©es
python-dotenv==1.0+       # Gestion des variables d'environnement
```

### Flux de communication

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     HTTP/REST      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Agent   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ Mistral         ‚îÇ
‚îÇ  ou Tool ‚îÇ                     ‚îÇ Connector       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                          ‚îÇ
                                          ‚îÇ SDK Mistral
                                          ‚îÇ
                                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                   ‚îÇ  Mistral AI ‚îÇ
                                   ‚îÇ     API     ‚îÇ
                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Conception du service

#### 1. **Configuration (`config.py`)**
G√®re les variables d'environnement via Pydantic Settings :
- Cl√© API Mistral
- Mod√®les par d√©faut
- Param√®tres CORS
- Configuration environnement

#### 2. **Mod√®les de donn√©es (`mistral_models.py`)**
D√©finit les sch√©mas Pydantic pour :
- Requ√™tes (ChatRequest, EmbeddingRequest)
- R√©ponses (ChatResponse, EmbeddingResponse)
- Validation automatique des donn√©es

#### 3. **Service m√©tier (`mistral_service.py`)**
Encapsule la logique d'interaction avec l'API Mistral :
- Initialisation du client Mistral
- Gestion des erreurs
- Transformation des r√©ponses

#### 4. **Routeur API (`mistral.py`)**
Expose les endpoints REST :
- `/api/v1/mistral/chat` - Chat completion
- `/api/v1/mistral/embeddings` - G√©n√©ration d'embeddings
- `/api/v1/mistral/models` - Liste des mod√®les

## üîå API REST

### Endpoints disponibles

#### **1. Chat Completion**

```http
POST /api/v1/mistral/chat
Content-Type: application/json
X-API-Key: optional_custom_key

{
  "messages": [
    {"role": "system", "content": "Tu es un assistant utile"},
    {"role": "user", "content": "Explique la photosynth√®se"}
  ],
  "model": "mistral-small-latest",
  "temperature": 0.7,
  "max_tokens": 1024,
  "top_p": 1.0,
  "safe_prompt": false
}
```

**R√©ponse:**
```json
{
  "success": true,
  "message": {
    "role": "assistant",
    "content": "La photosynth√®se est le processus...",
    "model": "mistral-small-latest"
  },
  "usage": {
    "prompt_tokens": 25,
    "completion_tokens": 150,
    "total_tokens": 175
  }
}
```

#### **2. Embeddings**

```http
POST /api/v1/mistral/embeddings
Content-Type: application/json

{
  "input": [
    "Premier texte √† vectoriser",
    "Deuxi√®me texte"
  ],
  "model": "mistral-embed"
}
```

**R√©ponse:**
```json
{
  "success": true,
  "embeddings": [
    [0.123, -0.456, 0.789, ...],
    [-0.321, 0.654, -0.987, ...]
  ],
  "model": "mistral-embed",
  "usage": {
    "total_tokens": 42
  }
}
```

#### **3. Liste des mod√®les**

```http
GET /api/v1/mistral/models
```

**R√©ponse:**
```json
{
  "success": true,
  "models": [
    {
      "id": "mistral-tiny",
      "description": "Rapide et √©conomique"
    },
    {
      "id": "mistral-small-latest",
      "description": "√âquilibr√© (par d√©faut)"
    }
  ]
}
```

#### **4. Health check**

```http
GET /health
```

**R√©ponse:**
```json
{
  "status": "healthy",
  "service": "mistral-connector",
  "version": "1.0.0",
  "mistral_configured": true
}
```

### Mod√®les disponibles

| Mod√®le | Tokens max | Cas d'usage | Co√ªt relatif |
|--------|-----------|-------------|--------------|
| `mistral-tiny` | 32K | Classification, extraction simple | ‚≠ê |
| `mistral-small-latest` | 32K | Usage g√©n√©ral, √©quilibr√© | ‚≠ê‚≠ê |
| `mistral-medium-latest` | 32K | T√¢ches complexes, raisonnement | ‚≠ê‚≠ê‚≠ê |
| `mistral-large-latest` | 32K | Raisonnement avanc√©, pr√©cision max | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `mistral-embed` | - | Vectorisation, recherche s√©mantique | ‚≠ê |

## üöÄ Utilisation

### Configuration

1. **Cr√©er le fichier `.env`**

```bash
cd core/mistral-connector
cp .env.example .env
```

2. **Configurer les variables**

```bash
# core/mistral-connector/.env
MISTRAL_API_KEY=your_mistral_api_key_here
ENVIRONMENT=production
CORS_ORIGINS=*
DEFAULT_MODEL=mistral-small-latest
DEFAULT_MAX_TOKENS=1024
DEFAULT_TEMPERATURE=0.7
```

3. **Obtenir une cl√© API Mistral**

Visitez https://console.mistral.ai/ et cr√©ez une cl√© API.

### D√©marrage

#### Via Docker Compose (recommand√©)

```bash
# Depuis la racine du projet
docker-compose up -d mistral-connector

# V√©rifier les logs
docker-compose logs -f mistral-connector

# Tester le service
curl http://localhost:8005/health
```

#### En d√©veloppement local

```bash
cd core/mistral-connector

# Installer les d√©pendances
pip install -r requirements.txt

# Lancer le serveur
uvicorn app.main:app --reload --port 8000

# Le service sera disponible sur http://localhost:8000
```

### Exemples d'int√©gration

#### Depuis Python

```python
import httpx

# Configuration
MISTRAL_CONNECTOR_URL = "http://localhost:8005"

# Chat simple
async def chat_simple():
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{MISTRAL_CONNECTOR_URL}/api/v1/mistral/chat",
            json={
                "messages": [
                    {"role": "user", "content": "Bonjour!"}
                ]
            }
        )
        result = response.json()
        if result["success"]:
            print(result["message"]["content"])
        else:
            print(f"Erreur: {result.get('error')}")

# Conversation multi-tours
async def chat_conversation():
    conversation = [
        {"role": "system", "content": "Tu es un expert en Python"},
        {"role": "user", "content": "C'est quoi une liste ?"},
    ]

    async with httpx.AsyncClient() as client:
        # Premier √©change
        response = await client.post(
            f"{MISTRAL_CONNECTOR_URL}/api/v1/mistral/chat",
            json={"messages": conversation}
        )
        result = response.json()

        # Ajouter la r√©ponse √† la conversation
        conversation.append(result["message"])
        conversation.append({
            "role": "user",
            "content": "Donne-moi un exemple"
        })

        # Deuxi√®me √©change
        response = await client.post(
            f"{MISTRAL_CONNECTOR_URL}/api/v1/mistral/chat",
            json={"messages": conversation}
        )

# G√©n√©ration d'embeddings
async def generate_embeddings():
    documents = [
        "Le chat dort sur le canap√©",
        "Le chien joue dans le jardin",
        "L'oiseau chante dans l'arbre"
    ]

    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{MISTRAL_CONNECTOR_URL}/api/v1/mistral/embeddings",
            json={"input": documents}
        )
        result = response.json()

        if result["success"]:
            embeddings = result["embeddings"]
            print(f"G√©n√©r√©s {len(embeddings)} vecteurs")
            print(f"Dimension: {len(embeddings[0])}")

# Avec cl√© API personnalis√©e
async def chat_with_custom_key():
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{MISTRAL_CONNECTOR_URL}/api/v1/mistral/chat",
            headers={"X-API-Key": "custom_mistral_key"},
            json={
                "messages": [{"role": "user", "content": "Hello"}]
            }
        )
```

#### Depuis un autre service FastAPI

```python
from fastapi import FastAPI, HTTPException
import httpx

app = FastAPI()

MISTRAL_URL = "http://mistral-connector:8000"

@app.post("/analyze-text")
async def analyze_text(text: str):
    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                f"{MISTRAL_URL}/api/v1/mistral/chat",
                json={
                    "messages": [
                        {
                            "role": "system",
                            "content": "Analyse le sentiment du texte"
                        },
                        {
                            "role": "user",
                            "content": text
                        }
                    ],
                    "temperature": 0.3
                },
                timeout=30.0
            )
            response.raise_for_status()
            return response.json()
        except httpx.HTTPError as e:
            raise HTTPException(
                status_code=500,
                detail=f"Erreur lors de l'appel √† Mistral: {str(e)}"
            )
```

#### Depuis JavaScript/TypeScript

```typescript
// Service Angular
import { Injectable } from '@angular/core';
import { HttpClient, HttpHeaders } from '@angular/common/http';
import { Observable } from 'rxjs';

interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface ChatRequest {
  messages: ChatMessage[];
  model?: string;
  temperature?: number;
  max_tokens?: number;
}

@Injectable({ providedIn: 'root' })
export class MistralService {
  private baseUrl = 'http://localhost:8005/api/v1/mistral';

  constructor(private http: HttpClient) {}

  chat(request: ChatRequest): Observable<any> {
    return this.http.post(`${this.baseUrl}/chat`, request);
  }

  embeddings(texts: string[]): Observable<any> {
    return this.http.post(`${this.baseUrl}/embeddings`, {
      input: texts
    });
  }

  chatWithCustomKey(request: ChatRequest, apiKey: string): Observable<any> {
    const headers = new HttpHeaders({
      'X-API-Key': apiKey
    });
    return this.http.post(`${this.baseUrl}/chat`, request, { headers });
  }
}

// Utilisation dans un composant
export class ChatComponent {
  constructor(private mistralService: MistralService) {}

  async sendMessage(userMessage: string) {
    const request: ChatRequest = {
      messages: [
        { role: 'user', content: userMessage }
      ],
      temperature: 0.7
    };

    this.mistralService.chat(request).subscribe({
      next: (response) => {
        if (response.success) {
          console.log('R√©ponse:', response.message.content);
        }
      },
      error: (error) => {
        console.error('Erreur:', error);
      }
    });
  }
}
```

## ‚öôÔ∏è Configuration avanc√©e

### Variables d'environnement

| Variable | Description | D√©faut | Obligatoire |
|----------|-------------|--------|-------------|
| `MISTRAL_API_KEY` | Cl√© API Mistral AI | - | ‚úÖ |
| `ENVIRONMENT` | Environnement (production/development) | production | ‚ùå |
| `CORS_ORIGINS` | Origines CORS autoris√©es (s√©par√©es par virgule) | * | ‚ùå |
| `DEFAULT_MODEL` | Mod√®le Mistral par d√©faut | mistral-small-latest | ‚ùå |
| `DEFAULT_MAX_TOKENS` | Limite de tokens par d√©faut | 1024 | ‚ùå |
| `DEFAULT_TEMPERATURE` | Temp√©rature par d√©faut | 0.7 | ‚ùå |

### Param√®tres de requ√™te

#### Chat Completion

| Param√®tre | Type | Description | D√©faut |
|-----------|------|-------------|--------|
| `messages` | Array | **Obligatoire**. Liste des messages | - |
| `model` | String | Mod√®le √† utiliser | mistral-small-latest |
| `temperature` | Float (0-2) | Contr√¥le la cr√©ativit√© | 0.7 |
| `max_tokens` | Integer | Limite de tokens g√©n√©r√©s | 1024 |
| `top_p` | Float (0-1) | Nucleus sampling | 1.0 |
| `safe_prompt` | Boolean | Active le mode safe prompt | false |

#### Embeddings

| Param√®tre | Type | Description | D√©faut |
|-----------|------|-------------|--------|
| `input` | Array[String] | **Obligatoire**. Textes √† vectoriser | - |
| `model` | String | Mod√®le d'embedding | mistral-embed |

### Gestion des erreurs

Le service retourne toujours une structure standardis√©e :

```json
{
  "success": false,
  "error": "Description de l'erreur",
  "error_type": "TypeException"
}
```

Codes HTTP retourn√©s :
- `200` : Succ√®s
- `400` : Erreur de validation des param√®tres
- `401` : Cl√© API invalide
- `500` : Erreur interne du serveur
- `503` : API Mistral indisponible

## üêõ Troubleshooting

### Probl√®mes courants

#### 1. Service ne d√©marre pas

**Sympt√¥mes:**
```bash
docker-compose ps
# mistral-connector: Exit 1
```

**Solutions:**
```bash
# V√©rifier les logs
docker-compose logs mistral-connector

# V√©rifier la cl√© API
cat .env | grep MISTRAL_API_KEY

# V√©rifier que le port 8005 est libre
netstat -tuln | grep 8005

# Rebuild
docker-compose up -d --build mistral-connector
```

#### 2. Erreur "Client Mistral non initialis√©"

**Cause:** Cl√© API non configur√©e ou invalide

**Solutions:**
```bash
# V√©rifier le .env
cat core/mistral-connector/.env

# Tester la cl√© manuellement
curl -H "Authorization: Bearer $MISTRAL_API_KEY" \
  https://api.mistral.ai/v1/models

# Recr√©er le .env
cp core/mistral-connector/.env.example core/mistral-connector/.env
nano core/mistral-connector/.env
```

#### 3. Timeout sur les requ√™tes

**Causes:** Requ√™te trop longue, max_tokens trop √©lev√©

**Solutions:**
```python
# Augmenter le timeout du client
async with httpx.AsyncClient(timeout=60.0) as client:
    response = await client.post(...)

# R√©duire max_tokens
{
    "messages": [...],
    "max_tokens": 500  # Au lieu de 2000
}
```

#### 4. Erreur CORS

**Sympt√¥mes:** Erreur dans le navigateur "CORS policy blocked"

**Solutions:**
```bash
# Configurer CORS_ORIGINS dans .env
CORS_ORIGINS=http://localhost:4200,http://localhost:3000

# Ou autoriser tout (d√©veloppement seulement)
CORS_ORIGINS=*

# Red√©marrer le service
docker-compose restart mistral-connector
```

#### 5. Erreur 429 (Rate Limit)

**Cause:** Trop de requ√™tes vers l'API Mistral

**Solutions:**
- Impl√©menter un syst√®me de retry avec backoff exponentiel
- R√©duire la fr√©quence des appels
- V√©rifier votre quota sur console.mistral.ai
- Passer √† un plan sup√©rieur si n√©cessaire

### Debugging

```bash
# Logs en temps r√©el
docker-compose logs -f mistral-connector

# Logs d√©taill√©s
docker-compose logs --tail=500 mistral-connector

# Acc√©der au container
docker-compose exec mistral-connector /bin/bash

# Tester depuis le container
docker-compose exec mistral-connector curl http://localhost:8000/health

# V√©rifier les variables d'environnement
docker-compose exec mistral-connector env | grep MISTRAL
```

## üîí S√©curit√©

### Bonnes pratiques impl√©ment√©es

1. ‚úÖ **Cl√© API s√©curis√©e** : Stock√©e dans .env, jamais committ√©e
2. ‚úÖ **Validation des entr√©es** : Pydantic pour toutes les requ√™tes
3. ‚úÖ **CORS configur√©** : Restreindre les origines en production
4. ‚úÖ **Logs structur√©s** : Tra√ßabilit√© des requ√™tes
5. ‚úÖ **Gestion d'erreurs** : Pas de fuite d'informations sensibles
6. ‚úÖ **User non-root** : Container Docker avec utilisateur limit√©

### Recommandations pour la production

- [ ] Utiliser HTTPS avec certificats SSL/TLS
- [ ] Restreindre CORS_ORIGINS aux domaines autoris√©s uniquement
- [ ] Impl√©menter un rate limiting
- [ ] Utiliser des secrets Docker pour MISTRAL_API_KEY
- [ ] Activer les logs JSON structur√©s
- [ ] Mettre en place un monitoring (Prometheus, Grafana)
- [ ] Configurer des alertes sur les erreurs
- [ ] Impl√©menter un circuit breaker pour l'API Mistral

## üìä Monitoring et observabilit√©

### M√©triques √† surveiller

- Nombre de requ√™tes par seconde
- Temps de r√©ponse moyen
- Taux d'erreurs (4xx, 5xx)
- Utilisation de tokens
- Disponibilit√© de l'API Mistral

### Health checks

```bash
# Health check simple
curl http://localhost:8005/health

# Health check avec d√©tails
curl http://localhost:8005/health | jq

# Int√©gration dans docker-compose.yml
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
  interval: 30s
  timeout: 3s
  retries: 3
```

## üìö Ressources

### Documentation officielle

- [Mistral AI Documentation](https://docs.mistral.ai/)
- [Mistral AI API Reference](https://docs.mistral.ai/api/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

### Liens internes

- [Documentation plateforme](../platform/PLATFORM.md)
- [Documentation agents](../agents/)
- [Guide d'architecture](../platform/PLATFORM.md#architecture)

---

**Service** : mistral-connector
**Port** : 8005
**Version** : 1.0.0
**Derni√®re mise √† jour** : Janvier 2026
