# Configuration Multi-LLM et D√©tection de Documents Professionnels

## üìã Vue d'ensemble

Ce document d√©crit les fonctionnalit√©s du systeme multi-LLM de la plateforme, gerees par l'Agent Runtime (port 8025). Deux problematiques principales sont adressees :

1. **Blocage injustifi√© de documents professionnels** (CCTP, RFP, etc.) par la mod√©ration
2. **Besoin de mod√®les LLM sp√©cialis√©s** selon le type de t√¢che (texte, document, image)

---

## üéØ Probl√®me R√©solu : Documents Professionnels Bloqu√©s

### Sympt√¥me
Les documents professionnels (CCTP, RFP, cahiers des charges) √©taient bloqu√©s par la mod√©ration car ils contenaient des mots comme "confidentiel", "secret", etc., consid√©r√©s √† tort comme suspects.

### Solution Impl√©ment√©e
1. **D√©tection automatique** des documents professionnels par nom de fichier et contenu
2. **Mod√©ration adapt√©e** qui comprend le contexte professionnel
3. **Patterns reconnus** : CCTP, RFP, RFI, RFQ, Appel d'offre, Cahier des charges, March√© public, etc.

### Exemple
```json
{
  "messages": [
    {
      "role": "user",
      "content": "Peux-tu analyser ce document ?",
      "documents": [
        {
          "name": "20255414 - CCTP AC solutions IA V2.pdf",
          "type": "application/pdf",
          "content": "..."
        }
      ]
    }
  ]
}
```

**R√©sultat** : Le document est automatiquement reconnu comme professionnel et la mod√©ration s'adapte ‚úÖ

---

## üöÄ Nouvelle Fonctionnalit√© : Configuration Multi-LLM

### Principe
Vous pouvez maintenant configurer **diff√©rents mod√®les LLM** pour diff√©rents types de t√¢ches :
- üí¨ **Texte/Prompt classique** ‚Üí Mod√®le l√©ger et rapide (Mistral Small, GPT-4o Mini)
- üìÑ **Analyse de documents** ‚Üí Mod√®le puissant (Mistral Large, GPT-4o)
- üñºÔ∏è **Analyse d'images** ‚Üí Mod√®le vision (Pixtral, GPT-4 Vision)

### Configuration par D√©faut (variables d'environnement)

#### Texte/Prompt Chat
```env
TEXT_CHAT_PROVIDER=mistral
TEXT_CHAT_MODEL=mistral-small-latest
TEXT_CHAT_TEMPERATURE=0.7
TEXT_CHAT_MAX_TOKENS=4096
```

#### Analyse de Documents
```env
DOCUMENT_ANALYSIS_PROVIDER=mistral
DOCUMENT_ANALYSIS_MODEL=mistral-large-latest
DOCUMENT_ANALYSIS_TEMPERATURE=0.3
DOCUMENT_ANALYSIS_MAX_TOKENS=8192
```

#### Analyse d'Images
```env
IMAGE_ANALYSIS_PROVIDER=mistral
IMAGE_ANALYSIS_MODEL=pixtral-12b-2409
IMAGE_ANALYSIS_TEMPERATURE=0.5
IMAGE_ANALYSIS_MAX_TOKENS=4096
```

### Configuration par Requ√™te (API)

Vous pouvez aussi configurer les LLM dynamiquement lors de chaque requ√™te :

```json
{
  "messages": [...],
  "multi_llm_config": {
    "text_chat": {
      "provider": "mistral",
      "model": "mistral-small-latest",
      "temperature": 0.7,
      "max_tokens": 4096
    },
    "document_analysis": {
      "provider": "mistral",
      "model": "mistral-large-latest",
      "temperature": 0.3,
      "max_tokens": 8192
    },
    "image_analysis": {
      "provider": "openai",
      "model": "gpt-4o",
      "temperature": 0.5,
      "max_tokens": 4096
    }
  },
  "task_type": "document_analysis"
}
```

### D√©tection Automatique du Type de T√¢che

Si vous ne sp√©cifiez pas `task_type`, le syst√®me d√©tecte automatiquement :
- Pr√©sence d'**images** ‚Üí `image_analysis`
- Pr√©sence de **documents** ‚Üí `document_analysis`
- Sinon ‚Üí `text_chat`

---

## üìä Mod√®les Recommand√©s

### Mistral
| T√¢che | Mod√®le | Pourquoi |
|-------|--------|----------|
| Texte classique | `mistral-small-latest` | Rapide, √©conomique, efficace |
| Documents complexes | `mistral-large-latest` | Meilleure compr√©hension, plus de contexte |
| Images | `pixtral-12b-2409` | Mod√®le vision natif Mistral |

### OpenAI
| T√¢che | Mod√®le | Pourquoi |
|-------|--------|----------|
| Texte classique | `gpt-4o-mini` | Rapide, √©conomique |
| Documents complexes | `gpt-4o` | Excellente compr√©hension |
| Images | `gpt-4o` | Vision int√©gr√©e |

### Anthropic
| T√¢che | Mod√®le | Pourquoi |
|-------|--------|----------|
| Texte classique | `claude-3-5-sonnet` | Raisonnement avanc√© |
| Documents complexes | `claude-3-5-sonnet` | Excellente analyse |

### Gemini
| T√¢che | Mod√®le | Pourquoi |
|-------|--------|----------|
| Texte classique | `gemini-2.0-flash-exp` | Rapide, multimodal |
| Documents complexes | `gemini-2.0-flash-exp` | Support natif documents |

### NVIDIA NIM
| T√¢che | Mod√®le | Pourquoi |
|-------|--------|----------|
| Texte classique | `llama-3.1-8b-instruct` | Haute performance |

### Ollama (inference locale)
| T√¢che | Mod√®le | Pourquoi |
|-------|--------|----------|
| Texte classique | `gemma3:4b` | Inference hors-ligne, confidentialite |

---

## Implementation technique

### Composants concernes

#### Agent Runtime (`agents/agent-runtime/`, port 8025)

Le moteur d'execution universel gere la configuration multi-LLM :
- `TaskType` enum : `text_chat`, `document_analysis`, `image_analysis`
- `LLMConfig` et `MultiLLMConfig` : modeles de configuration par type de tache
- Detection automatique du type de tache selon le contenu de la requete
- Resolution dynamique du connecteur LLM a utiliser
- Detection des documents professionnels (CCTP, RFP, etc.)
- Transmission du contexte professionnel a la moderation

#### Moderation (`tools/prompt-moderation-tool/`, port 8013)
- Champs de requete : `is_professional_document`, `document_type`, `has_attachments`
- Logique de moderation adaptee au contexte professionnel
- Patterns de confidentialite relaxes pour documents professionnels

#### Connecteurs LLM disponibles
- Mistral (port 8005), OpenAI (port 8006), Anthropic (port 8024)
- Gemini (port 8023), Perplexity (port 8022), NVIDIA NIM (port 8028)
- Ollama (port 8040) pour l'inference locale

---

## üß™ Exemples d'Utilisation

### Exemple 1 : Document CCTP avec Auto-D√©tection
```json
{
  "messages": [
    {
      "role": "user",
      "content": "Analyse ce CCTP et donne-moi les points cl√©s",
      "documents": [
        {
          "name": "CCTP_Projet_IA.pdf",
          "type": "application/pdf",
          "content": "..."
        }
      ]
    }
  ]
}
```

**R√©sultat** :
- ‚úÖ Document reconnu comme CCTP
- ‚úÖ Mod√©ration adapt√©e (pas de blocage pour "confidentiel")
- ‚úÖ Utilise `mistral-large-latest` automatiquement

### Exemple 2 : Image avec Configuration Personnalis√©e
```json
{
  "messages": [
    {
      "role": "user",
      "content": "Que vois-tu sur cette image ?",
      "images": ["data:image/jpeg;base64,..."]
    }
  ],
  "multi_llm_config": {
    "image_analysis": {
      "provider": "openai",
      "model": "gpt-4o",
      "temperature": 0.3,
      "max_tokens": 2048
    }
  }
}
```

**R√©sultat** :
- ‚úÖ D√©tection automatique : `task_type = image_analysis`
- ‚úÖ Utilise GPT-4o avec vision
- ‚úÖ Configuration personnalis√©e appliqu√©e

### Exemple 3 : Texte Simple
```json
{
  "messages": [
    {
      "role": "user",
      "content": "Explique-moi le concept de microservices"
    }
  ]
}
```

**R√©sultat** :
- ‚úÖ Utilise la config par d√©faut : `mistral-small-latest`
- ‚úÖ Mod√©ration standard
- ‚úÖ Rapide et √©conomique

---

## üéì Recommandations

### Quand Utiliser Quel Mod√®le ?

#### Mistral Small (Texte)
- Questions simples
- Chat conversationnel
- Traductions basiques
- Code simple

#### Mistral Large (Documents)
- Documents techniques (CCTP, RFP)
- Analyses complexes
- Synth√®ses de documents longs
- Raisonnement avanc√©

#### Pixtral (Images)
- Analyse de sch√©mas
- Extraction de texte d'images
- Descriptions visuelles
- OCR et reconnaissance

---

## üìù Notes Importantes

1. **R√©trocompatibilit√©** : L'ancienne configuration (`provider`, `model`, `temperature`) fonctionne toujours
2. **Auto-d√©tection** : Le syst√®me choisit le bon mod√®le si vous ne sp√©cifiez pas `task_type`
3. **Documents Professionnels** : Reconnaissance automatique par nom de fichier
4. **Mod√©ration Intelligente** : S'adapte au contexte professionnel

---

## üêõ R√©solution de Probl√®mes

### Mon document CCTP est toujours bloqu√©
- ‚úÖ V√©rifiez que le nom du fichier contient "CCTP" ou "cahier des clauses"
- ‚úÖ Ou ajoutez `"is_professional_document": true` dans la requ√™te

### Le mauvais mod√®le est utilis√©
- ‚úÖ Sp√©cifiez explicitement `task_type` dans la requ√™te
- ‚úÖ Ou utilisez `multi_llm_config` pour forcer un mod√®le

### Erreur "model not found"
- ‚úÖ V√©rifiez que le mod√®le existe chez le provider
- ‚úÖ Mistral : `mistral-small-latest`, `mistral-large-latest`, `pixtral-12b-2409`
- ‚úÖ OpenAI : `gpt-4o-mini`, `gpt-4o`

---

## Support

Pour toute question ou probleme :
1. Consultez ce document
2. Consultez les logs de l'Agent Runtime : `docker-compose logs -f agent-runtime`
3. Consultez la documentation dans `docs/`
